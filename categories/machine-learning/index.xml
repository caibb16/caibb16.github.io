<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Machine Learning on caibb16</title>
        <link>http://localhost:1313/categories/machine-learning/</link>
        <description>Recent content in Machine Learning on caibb16</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>caibb16</copyright>
        <lastBuildDate>Sun, 21 Dec 2025 14:49:14 +0800</lastBuildDate><atom:link href="http://localhost:1313/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Reinforcement Learning</title>
        <link>http://localhost:1313/p/reinforcement-learning/</link>
        <pubDate>Sun, 21 Dec 2025 14:49:14 +0800</pubDate>
        
        <guid>http://localhost:1313/p/reinforcement-learning/</guid>
        <description>&lt;h2 id=&#34;什么是强化学习&#34;&gt;什么是强化学习
&lt;/h2&gt;&lt;p&gt;强化学习（Reinforcement Learning，RL）是一种机器学习范式，旨在训练智能体通过与环境交互来学习最佳行为策略，以最大化累积奖励。与监督学习不同，强化学习没有明确的输入输出对，而是通过试错和奖励信号来指导学习过程。&lt;/p&gt;
&lt;h2 id=&#34;强化学习的基本概念&#34;&gt;强化学习的基本概念
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;智能体（Agent）&lt;/strong&gt;：执行动作以与环境交互的实体。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;环境（Environment）&lt;/strong&gt;：智能体所处的外部系统，智能体通过与环境交互来获取状态和奖励。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;状态（State）&lt;/strong&gt;：环境在某一时刻的描述，智能体根据状态做出决策。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动作（Action）&lt;/strong&gt;：智能体在特定状态下可以执行的操作。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;奖励（Reward）&lt;/strong&gt;：智能体执行动作后从环境中获得的反馈信号，用于评估动作的好坏。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;策略（Policy）&lt;/strong&gt;：智能体在给定状态下选择动作的规则或函数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;价值函数（Value Function）&lt;/strong&gt;：评估在某一状态下，智能体未来可能获得的累积奖励。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;马尔可夫决策过程&#34;&gt;马尔可夫决策过程
&lt;/h2&gt;&lt;p&gt;强化学习通常建模为马尔可夫决策过程（Markov Decision Process，MDP），包括状态空间、动作空间、转移概率和奖励函数。MDP假设未来状态仅依赖于当前状态和动作，而与过去状态无关。&lt;/p&gt;
&lt;h2 id=&#34;ppo算法简介&#34;&gt;PPO算法简介
&lt;/h2&gt;&lt;p&gt;Proximal Policy Optimization（PPO）是一种常用的强化学习算法，属于策略梯度方法。PPO通过限制策略更新的幅度，确保新策略不会偏离旧策略过远，从而提高训练的稳定性和效率。PPO通常使用剪切概率比（clipped probability ratio）来限制策略更新。&lt;/p&gt;
&lt;h3 id=&#34;基本架构&#34;&gt;基本架构
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;策略网络（Policy Network）&lt;/strong&gt;：用于输出在给定状态下选择各个动作的概率分布。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;价值网络（Value Network）&lt;/strong&gt;：用于估计在给定状态下的价值函数。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;训练过程&#34;&gt;训练过程
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;采样数据&lt;/strong&gt;：智能体与环境交互，策略网络根据当前状态 s 和自身策略，选择并执行动作 a，收集状态、动作、奖励和下一个状态的数据。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算优势函数&lt;/strong&gt;：使用价值网络估计状态的价值，并计算优势函数以评估动作的好坏。优势用于衡量在给定状态下选择某个动作相对于平均水平的好处，其计算方法通常是用动作价值函数减去状态价值函数。公式如下：
 &lt;center&gt;A(s, a) = Q(s, a) - V(s)&lt;/center&gt; 
 其中， A(s, a)是优势函数，Q(s, a)表示在状态 s 下采取动作 a 随后继续遵循策略π的价值，V(s) 表示在状态 s 下遵循策略π的价值。
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;更新策略网络&lt;/strong&gt;：利用优势函数来更新自己的策略网络，使自己更倾向于选择能获得高评分的动作。同时使用剪切概率比限制策略更新的幅度，确保新策略不会偏离旧策略过远。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;更新价值网络&lt;/strong&gt;：使用TD误差优化价值网络参数，TD误差可以衡量价值网络预测值与实际回报之间的差异。&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        
    </channel>
</rss>
